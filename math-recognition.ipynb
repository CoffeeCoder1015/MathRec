{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3307,"sourceType":"datasetVersion","datasetId":1916}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nroot = \"/kaggle/input/hasyv2-dataset-friend-of-mnist/HASYv2\"\n\nprint(pd.read_csv(root+\"/hasy-data-labels.csv\")[\"latex\"].unique())\n\nstart = root+\"/classification-task/fold-1\"\n\ntrain_csv = pd.read_csv(start+\"/train.csv\")\ntest_csv = pd.read_csv(start+\"/test.csv\")\n\nclasses = train_csv[\"latex\"].unique()\nhashed_class = {v:i for i,v in enumerate(classes)}\nspan = len(classes)+1\nprint(span,\"many symbols\")\n\ndef get_target_ids(symbols):\n    return np.array([hashed_class[s] for s in symbols])\n    \ndef get_targets(symbols):\n    base = np.zeros(shape=(len(symbols),span))\n    for i,s in enumerate(symbols):\n        idx = hashed_class[s]\n        base[i][idx] = 1\n    return base\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:03:31.184968Z","iopub.execute_input":"2025-11-12T08:03:31.185660Z","iopub.status.idle":"2025-11-12T08:03:31.554431Z","shell.execute_reply.started":"2025-11-12T08:03:31.185638Z","shell.execute_reply":"2025-11-12T08:03:31.553624Z"}},"outputs":[{"name":"stdout","text":"['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R'\n 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' '\\\\rightarrow' '0' '1' '2' '3' '4' '5'\n '6' '7' '8' '9' '\\\\pi' '\\\\alpha' '\\\\beta' '\\\\sum' '\\\\sigma' 'a' 'b' 'c'\n 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 'u' 'v'\n 'w' 'x' 'y' 'z' '\\\\Sigma' '\\\\gamma' '\\\\Gamma' '\\\\delta' '\\\\Delta'\n '\\\\zeta' '\\\\eta' '\\\\theta' '\\\\Theta' '\\\\epsilon' '\\\\varepsilon' '\\\\iota'\n '\\\\kappa' '\\\\varkappa' '\\\\lambda' '\\\\Lambda' '\\\\mu' '\\\\nu' '\\\\xi' '\\\\Xi'\n '\\\\Pi' '\\\\rho' '\\\\varrho' '\\\\tau' '\\\\phi' '\\\\Phi' '\\\\varphi' '\\\\chi'\n '\\\\psi' '\\\\Psi' '\\\\omega' '\\\\Omega' '\\\\partial' '\\\\int' '\\\\cdot' '\\\\leq'\n '\\\\geq' '<' '>' '\\\\subset' '\\\\supset' '\\\\subseteq' '\\\\supseteq' '\\\\cong'\n '\\\\propto' '-' '+' '\\\\mathbb{R}' '\\\\$' '\\\\{' '\\\\copyright' '\\\\dots' '\\\\}'\n '\\\\S' '\\\\dag' '\\\\pounds' '\\\\&' '\\\\#' '\\\\%' '\\\\checkmark' '\\\\circledR'\n '\\\\mathsection' '\\\\amalg' '\\\\cup' '\\\\oplus' '\\\\times' '\\\\ast'\n '\\\\triangleleft' '\\\\otimes' '\\\\triangleright' '\\\\diamond' '\\\\pm' '\\\\div'\n '\\\\bullet' '\\\\setminus' '\\\\uplus' '\\\\cap' '\\\\mp' '\\\\sqcap' '\\\\vee'\n '\\\\odot' '\\\\sqcup' '\\\\wedge' '\\\\circ' '\\\\ominus' '\\\\star' '\\\\wr'\n '\\\\barwedge' '\\\\circledcirc' '\\\\boxdot' '\\\\ltimes' '\\\\boxplus'\n '\\\\boxtimes' '\\\\rtimes' '\\\\circledast' '\\\\lhd' '\\\\prod' '\\\\coprod'\n '\\\\oint' '\\\\parr' '\\\\with' '\\\\fint' '\\\\varoiint' '\\\\oiint' '\\\\approx'\n '\\\\equiv' '\\\\not\\\\equiv' '\\\\perp' '\\\\asymp' '\\\\frown' '\\\\prec' '\\\\succ'\n '\\\\bowtie' '\\\\preceq' '\\\\succeq' '\\\\mid' '\\\\vdash' '\\\\dashv' '\\\\models'\n '\\\\sim' '\\\\doteq' '\\\\parallel' '\\\\simeq' '\\\\backsim' '\\\\multimap'\n '\\\\pitchfork' '\\\\therefore' '\\\\because' '\\\\between' '\\\\preccurlyeq'\n '\\\\varpropto' '\\\\Vdash' '\\\\vDash' '\\\\nmid' '\\\\nvDash' '\\\\sqsubseteq'\n '\\\\nsubseteq' '\\\\subsetneq' '\\\\varsubsetneq' '\\\\neq' '\\\\geqslant'\n '\\\\gtrless' '\\\\lesssim' '\\\\gtrsim' '\\\\leqslant' '\\\\trianglelefteq'\n '\\\\blacktriangleright' '\\\\triangleq' '\\\\Downarrow' '\\\\downarrow'\n '\\\\Rightarrow' '\\\\hookrightarrow' '\\\\Longleftrightarrow' '\\\\searrow'\n '\\\\longmapsto' '\\\\leftarrow' '\\\\Longrightarrow' '\\\\uparrow' '\\\\Leftarrow'\n '\\\\longrightarrow' '\\\\Leftrightarrow' '\\\\mapsto' '\\\\leftrightarrow'\n '\\\\nearrow' '\\\\rightleftharpoons' '\\\\rightharpoonup' '\\\\leadsto'\n '\\\\circlearrowleft' '\\\\rightleftarrows' '\\\\circlearrowright'\n '\\\\rightrightarrows' '\\\\rightsquigarrow' '\\\\curvearrowright'\n '\\\\twoheadrightarrow' '\\\\nRightarrow' '\\\\nrightarrow' '\\\\upharpoonright'\n '\\\\mapsfrom' '\\\\shortrightarrow' '\\\\lightning' '\\\\vartheta' '\\\\varpi'\n '\\\\bot' '\\\\forall' '\\\\ni' '\\\\top' '\\\\ell' '\\\\hbar' '\\\\in' '\\\\notin'\n '\\\\wp' '\\\\exists' '\\\\Im' '\\\\Re' '\\\\nexists' '\\\\langle' '\\\\rangle'\n '\\\\lceil' '\\\\rceil' '\\\\lfloor' '\\\\rfloor' '[' ']' '|' '\\\\|' '/'\n '\\\\llbracket' '\\\\rrbracket' '\\\\vdots' '\\\\ddots' '\\\\dotsc' '\\\\aleph'\n '\\\\infty' '\\\\prime' '\\\\angle' '\\\\diamondsuit' '\\\\sharp' '\\\\backslash'\n '\\\\emptyset' '\\\\nabla' '\\\\flat' '\\\\clubsuit' '\\\\heartsuit' '\\\\neg'\n '\\\\triangle' '\\\\sqrt{}' '\\\\sphericalangle' '\\\\square' '\\\\triangledown'\n '\\\\blacksquare' '\\\\lozenge' '\\\\varnothing' '\\\\vartriangle' '\\\\iddots'\n '\\\\mathcal{A}' '\\\\mathcal{B}' '\\\\mathcal{C}' '\\\\mathcal{D}'\n '\\\\mathcal{E}' '\\\\mathcal{F}' '\\\\mathcal{G}' '\\\\mathcal{H}'\n '\\\\mathcal{L}' '\\\\mathcal{M}' '\\\\mathcal{N}' '\\\\mathcal{O}'\n '\\\\mathcal{P}' '\\\\mathcal{R}' '\\\\mathcal{S}' '\\\\mathcal{T}'\n '\\\\mathcal{U}' '\\\\mathcal{X}' '\\\\mathcal{Z}' '\\\\mathfrak{A}'\n '\\\\mathfrak{M}' '\\\\mathfrak{S}' '\\\\mathfrak{X}' '\\\\mathbb{1}'\n '\\\\mathds{1}' '\\\\mathds{C}' '\\\\mathds{E}' '\\\\mathds{N}' '\\\\mathds{P}'\n '\\\\mathds{Q}' '\\\\mathds{R}' '\\\\mathds{Z}' '\\\\mathscr{A}' '\\\\mathscr{C}'\n '\\\\mathscr{D}' '\\\\mathscr{E}' '\\\\mathscr{F}' '\\\\mathscr{H}'\n '\\\\mathscr{L}' '\\\\mathscr{P}' '\\\\mathscr{S}' '\\\\celsius' '\\\\degree'\n '\\\\ohm' '\\\\venus' '\\\\mars' '\\\\astrosun' '\\\\fullmoon' '\\\\leftmoon'\n '\\\\female' '\\\\male' '\\\\checked' '\\\\diameter' '\\\\sun' '\\\\Bowtie'\n '\\\\mathbb{Q}' '\\\\mathbb{N}' '\\\\mathbb{Z}' '\\\\mathbb{H}' '\\\\ss' '\\\\aa'\n '\\\\L' '\\\\AA' '\\\\O' '\\\\o' '\\\\ae' '\\\\AE' '\\\\guillemotleft']\n370 many symbols\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Converts to [C, H, W], scales 0-255 to 0-1\n    # Optionally normalize to mean=0.5, std=0.5 (common for grayscale)\n    transforms.Normalize((0.5,), (0.5,))\n])\n\nclass HASYDataset(Dataset):\n    def __init__(self, train_csv_df):\n        self.csv = train_csv_df\n\n    def __len__(self):\n        return len(self.csv)\n\n    def __getitem__(self, idx):\n        obj = self.csv.iloc[idx]\n        path = start+\"/\"+obj[\"path\"]\n        label = obj[\"latex\"]\n        image = transform(Image.open(path).convert(\"L\"))\n        return image,label\n\nTrainData = HASYDataset(train_csv)\n\ndataloader = DataLoader(TrainData, batch_size=32, shuffle=True, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:03:31.555598Z","iopub.execute_input":"2025-11-12T08:03:31.555811Z","iopub.status.idle":"2025-11-12T08:03:31.569815Z","shell.execute_reply.started":"2025-11-12T08:03:31.555794Z","shell.execute_reply":"2025-11-12T08:03:31.569245Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"# --- NEW: SAVE AND LOAD FUNCTIONS ---\ndef save_checkpoint(model, path):\n    \"\"\"Saves only the model's state_dict.\"\"\"\n    torch.save(model.state_dict(), path)\n    print(f\"\\n--- Weights successfully saved to {path} ---\")\n\ndef load_checkpoint(model, path, device):\n    \"\"\"Loads weights from path into the model.\"\"\"\n    if os.path.exists(path):\n        # Load the weights, mapping them to the current device (CPU or GPU)\n        model.load_state_dict(torch.load(path, map_location=device))\n        print(f\"--- Weights successfully loaded from {path} ---\")\n    else:\n        print(f\"--- Checkpoint file not found at {path}. Starting from scratch. ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:03:31.570444Z","iopub.execute_input":"2025-11-12T08:03:31.570624Z","iopub.status.idle":"2025-11-12T08:03:31.587427Z","shell.execute_reply.started":"2025-11-12T08:03:31.570609Z","shell.execute_reply":"2025-11-12T08:03:31.586760Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch.optim as optim\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef conv_block(chan_in, chan_out, kern=3):\n    return nn.Sequential(\n        nn.Conv2d(chan_in, chan_out, kern, padding=kern//2), # <-- Stride is 1 by default\n        nn.BatchNorm2d(chan_out), # <-- Added for stability and speed\n        nn.ReLU()\n    )\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.net = nn.Sequential(\n            conv_block(1, 128), # 32x32\n            nn.MaxPool2d(2, 2), # 16x16\n            conv_block(128, 256), # 16x16\n            nn.MaxPool2d(2, 2), # 8x8\n            conv_block(256, 64), # 8x8\n            nn.MaxPool2d(2, 2), # 4x4\n            conv_block(64, 96), # 4x4\n            conv_block(96, 96), # 4x4\n            nn.Conv2d(96, 32, 1), # 1x1 convolution to reduce channels\n            nn.Flatten(), # 32 * 4 * 4 = 512\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.1), # <-- Crucial regularization\n            nn.Linear(1024, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.1), # <-- Crucial regularization\n            nn.Linear(1024, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.1), # <-- Crucial regularization\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1), # <-- Crucial regularization\n            nn.Linear(512,span)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\ncriterion = nn.CrossEntropyLoss()\nbs = SimpleNet()\nbs.to(device)\n# ðŸ› FIXED: Using a lower, more stable learning rate (0.001)\nLEARNING_RATE = 0.001\noptimizer = optim.Adam(bs.parameters(), lr=LEARNING_RATE)\nSPAN = span\n\nNUM_EPOCHS = 10\n# Use a scheduler to gradually decrease LR over the whole training run\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, \n    T_max=len(dataloader) * NUM_EPOCHS\n)\n\n# --- 5. ADVANCED TRAINING LOOP ---\n\nprint(f\"--- Starting Training ---\")\nprint(f\"Model: SimpleNet (Classes: {SPAN}, Initial LR: {LEARNING_RATE})\")\n\nfor epoch in range(NUM_EPOCHS):\n    bs.train() # Set model to training mode\n    running_loss = 0.0\n    total_samples = 0\n    correct_predictions = 0\n    \n    # Iterate over the data loader\n    for batch_idx, (image, label) in enumerate(dataloader):\n        \n        # 1. Prepare Data\n        optimizer.zero_grad()\n        output = bs(image.to(device))\n        \n        # Convert label (string list) to integer array, then to a Long Tensor\n        # ðŸ› FIXED: Ensures targets are torch.long for CrossEntropyLoss\n        targets = torch.from_numpy(get_target_ids(label)).long().to(device)\n        \n        # 2. Forward Pass & Loss\n        loss = criterion(output, targets)\n        \n        # 3. Backward Pass & Optimization\n        loss.backward()\n        optimizer.step()\n        scheduler.step() # Update learning rate\n\n        # 4. Statistics Tracking\n        running_loss += loss.item() * image.size(0)\n        \n        _, predicted = torch.max(output.data, 1)\n        total_samples += targets.size(0)\n        correct_predictions += (predicted == targets).sum().item()\n\n        # Report every 100 batches (or less if the dataset is small)\n        REPORT_FREQ = 20\n        if (batch_idx + 1) % REPORT_FREQ == 0 or (batch_idx + 1) == len(dataloader):\n            current_loss = running_loss / total_samples\n            current_accuracy = 100 * correct_predictions / total_samples\n            \n            print(f'Epoch [{epoch+1}/{NUM_EPOCHS}] | '\n                  f'Batch [{batch_idx+1}/{len(dataloader)}] | '\n                  f'Loss: {current_loss:.4f} | '\n                  f'Acc: {current_accuracy:.2f}% | '\n                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}',end='\\r')\n            \n    # Calculate and report end-of-epoch statistics\n    epoch_loss = running_loss / total_samples\n    epoch_accuracy = 100 * correct_predictions / total_samples\n    \n    print(f'\\n--- EPOCH {epoch+1} SUMMARY ---')\n    print(f'Average Loss: {epoch_loss:.4f}')\n    print(f'Accuracy: {epoch_accuracy:.2f}%\\n')\n\nprint(\"--- Training Finished ---\")\nWEIGHTS_PATH = '/kaggle/working/simplenet_weights.pth'\nsave_checkpoint(bs, WEIGHTS_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:03:31.588907Z","iopub.execute_input":"2025-11-12T08:03:31.589147Z","iopub.status.idle":"2025-11-12T08:20:03.540949Z","shell.execute_reply.started":"2025-11-12T08:03:31.589132Z","shell.execute_reply":"2025-11-12T08:20:03.540191Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n--- Starting Training ---\nModel: SimpleNet (Classes: 370, Initial LR: 0.001)\nEpoch [1/10] | Batch [4727/4727] | Loss: 1.6493 | Acc: 59.87% | LR: 0.000976\n--- EPOCH 1 SUMMARY ---\nAverage Loss: 1.6493\nAccuracy: 59.87%\n\nEpoch [2/10] | Batch [4727/4727] | Loss: 0.8157 | Acc: 76.65% | LR: 0.000905\n--- EPOCH 2 SUMMARY ---\nAverage Loss: 0.8157\nAccuracy: 76.65%\n\nEpoch [3/10] | Batch [4727/4727] | Loss: 0.6807 | Acc: 79.68% | LR: 0.000794\n--- EPOCH 3 SUMMARY ---\nAverage Loss: 0.6807\nAccuracy: 79.68%\n\nEpoch [4/10] | Batch [4727/4727] | Loss: 0.5937 | Acc: 81.56% | LR: 0.000655\n--- EPOCH 4 SUMMARY ---\nAverage Loss: 0.5937\nAccuracy: 81.56%\n\nEpoch [5/10] | Batch [4727/4727] | Loss: 0.5231 | Acc: 83.21% | LR: 0.000500\n--- EPOCH 5 SUMMARY ---\nAverage Loss: 0.5231\nAccuracy: 83.21%\n\nEpoch [6/10] | Batch [4727/4727] | Loss: 0.4585 | Acc: 84.56% | LR: 0.000345\n--- EPOCH 6 SUMMARY ---\nAverage Loss: 0.4585\nAccuracy: 84.56%\n\nEpoch [7/10] | Batch [4727/4727] | Loss: 0.4030 | Acc: 85.84% | LR: 0.000206\n--- EPOCH 7 SUMMARY ---\nAverage Loss: 0.4030\nAccuracy: 85.84%\n\nEpoch [8/10] | Batch [4727/4727] | Loss: 0.3561 | Acc: 87.03% | LR: 0.000095\n--- EPOCH 8 SUMMARY ---\nAverage Loss: 0.3561\nAccuracy: 87.03%\n\nEpoch [9/10] | Batch [4727/4727] | Loss: 0.3242 | Acc: 87.83% | LR: 0.000024\n--- EPOCH 9 SUMMARY ---\nAverage Loss: 0.3242\nAccuracy: 87.83%\n\nEpoch [10/10] | Batch [4727/4727] | Loss: 0.3081 | Acc: 88.27% | LR: 0.000000\n--- EPOCH 10 SUMMARY ---\nAverage Loss: 0.3081\nAccuracy: 88.27%\n\n--- Training Finished ---\n\n--- Weights successfully saved to /kaggle/working/simplenet_weights.pth ---\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"TestData = HASYDataset(test_csv)\ntest_dataloader = DataLoader(TestData, batch_size=64, shuffle=False, num_workers=2) # Larger batch size for testing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:22:48.415085Z","iopub.execute_input":"2025-11-12T08:22:48.415362Z","iopub.status.idle":"2025-11-12T08:22:48.419436Z","shell.execute_reply.started":"2025-11-12T08:22:48.415342Z","shell.execute_reply":"2025-11-12T08:22:48.418822Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# --- 6. DEDICATED TESTING LOOP ---\n\ndef test_loop(model, dataloader, criterion):\n    \"\"\"\n    Evaluates the model on the test dataset and prints final metrics.\n    \"\"\"\n    model.eval() # Set model to evaluation mode\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    print(\"--- Starting Testing ---\")\n\n    # Disable gradient calculations during evaluation\n    with torch.no_grad():\n        for image, label in dataloader:\n            \n            # 3. Move input data and targets to the device\n            image = image.to(device)\n\n            # Prepare targets\n            targets = torch.from_numpy(get_target_ids(label)).long().to(device) \n            \n            # Forward pass\n            output = model(image)\n            \n            # Calculate loss and statistics\n            test_loss += criterion(output, targets).item() * image.size(0)\n            _, predicted = torch.max(output.data, 1)\n            total += targets.size(0)\n            # Use .cpu() for accurate total sum on CPU\n            correct += (predicted == targets).sum().item()\n\n    # Calculate final metrics\n    avg_loss = test_loss / total\n    accuracy = 100 * correct / total\n    \n    print(f'\\n======================================')\n    print(f'FINAL TEST RESULTS ({total} samples):')\n    print(f'Test Loss: {avg_loss:.4f}')\n    print(f'Test Accuracy: {accuracy:.2f}%')\n    print(f'======================================\\n')\n\n\n# Run the test loop on the fully trained model\ntest_loop(bs, test_dataloader, criterion)\n\n# Clean up mock data (optional)\n# import shutil\n# if os.path.exists(start):\n#     shutil.rmtree(start)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:22:50.227605Z","iopub.execute_input":"2025-11-12T08:22:50.228109Z","iopub.status.idle":"2025-11-12T08:22:59.704041Z","shell.execute_reply.started":"2025-11-12T08:22:50.228085Z","shell.execute_reply":"2025-11-12T08:22:59.703100Z"}},"outputs":[{"name":"stdout","text":"--- Starting Testing ---\n\n======================================\nFINAL TEST RESULTS (16992 samples):\nTest Loss: 0.4748\nTest Accuracy: 84.70%\n======================================\n\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"from IPython.display import Image as displayImage\ndef predict_single_image(model, image_path, device):\n    \"\"\"\n    Loads a single image, processes it, and returns the model's prediction, \n    confidence, and the top 5 predictions.\n    \"\"\"\n    \n    # 1. Load and Transform Image\n    try:\n        # Load as grayscale (L) and apply the same transformations as training\n        image = Image.open(path).convert(\"L\")\n        input_tensor = transform(image)\n        \n        # Add a batch dimension: [C, H, W] -> [1, C, H, W]\n        input_batch = input_tensor.unsqueeze(0) \n    except FileNotFoundError:\n        print(f\"Error: Image file not found at {os.path.join(start, image_path)}\")\n        return\n    except Exception as e:\n        print(f\"Error loading or transforming image: {e}\")\n        return\n\n    # 2. Move to Device and Set to Evaluation Mode\n    input_batch = input_batch.to(device)\n    model.eval()\n\n    # 3. Generate Prediction\n    with torch.no_grad():\n        output = model(input_batch)\n\n    # 4. Process Output\n    # Convert logits to probabilities using Softmax\n    probabilities = nn.functional.softmax(output, dim=1)\n    \n    # Get the top prediction and its confidence\n    confidence, predicted_class = torch.max(probabilities, 1)\n    \n    # Get the top 5 predictions\n    top5_confidences, top5_indices = torch.topk(probabilities, 5)\n\n    # Convert tensors to standard Python values/lists\n    predicted_class_id = predicted_class.item()\n    confidence_value = confidence.item()\n\n    print(f'\\n--- SINGLE IMAGE PREDICTION RESULTS ---')\n    print(f'Input Image: {image_path}')\n    print(f'Predicted Class ID: {predicted_class_id} / {classes[predicted_class_id]}')\n    print(f'Confidence: {confidence_value:.4f}')\n    print(f'\\nTop 5 Predictions:')\n    for i in range(5):\n        class_id = top5_indices[0][i].item()\n        conf = top5_confidences[0][i].item()\n        print(f'  {i+1}. Class ID {class_id} / {classes[class_id]}: Confidence {conf:.4f}')\n\n# Run single image prediction\nidx = test_csv.iloc[5600]\npath = start+\"/\"+idx[\"path\"]\n\n# predict_single_image(bs,path,device)\npredict_single_image(bs, path, device)\nprint(\"Actual:\",idx[\"latex\"])\nprint(\"Image:\")\ndisplayImage(path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:23:03.322357Z","iopub.execute_input":"2025-11-12T08:23:03.323145Z","iopub.status.idle":"2025-11-12T08:23:03.340614Z","shell.execute_reply.started":"2025-11-12T08:23:03.323114Z","shell.execute_reply":"2025-11-12T08:23:03.340042Z"}},"outputs":[{"name":"stdout","text":"\n--- SINGLE IMAGE PREDICTION RESULTS ---\nInput Image: /kaggle/input/hasyv2-dataset-friend-of-mnist/HASYv2/classification-task/fold-1/../../hasy-data/v2-05632.png\nPredicted Class ID: 18 / \\$\nConfidence: 1.0000\n\nTop 5 Predictions:\n  1. Class ID 18 / \\$: Confidence 1.0000\n  2. Class ID 171 / \\%: Confidence 0.0000\n  3. Class ID 21 / \\&: Confidence 0.0000\n  4. Class ID 143 / \\xi: Confidence 0.0000\n  5. Class ID 344 / \\mathscr{L}: Confidence 0.0000\nActual: \\$\nImage:\n","output_type":"stream"},{"execution_count":92,"output_type":"execute_result","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAAsklEQVR4nO2VSw6AMAhEwXj/K+NC46JWmJmq0cSuTKS88hnwiDD4uLuZUVcm3FQ7c/57fbKRr0YBu/fmmzpKitwd5/lZ7N3kHP2WqSsi6N5/ootwRtFF44zbdcABBDWIbXoZgGp5DtC0v0zKIjhmXGCcKrljysvYqCJHROMRCWhUyWVtdKHlQ/ACAMgYHRX6uEYKiIgDXZkCfrPMY0wcgQIq9sG4mOkisxP7ZQvnB3wSsACYUEtQBaFttAAAAABJRU5ErkJggg==\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}],"execution_count":92}]}